# SVM

​	可用于分类，处理分类问题的最好ML算法。

## 1、原理

​	找到一个超平面，尽可能多地将两类数据点分开，并使分开的数据点离平面尽可能远。

- 训练集：$\{[a_i,yi]|i=0,1,2...\}$
- 输入空间：$a_i\in R^n$
- 输出空间：$y_i\in \{1，-1\}$
- 函数：$f(x)=sgn(g(x))$
- 

## 2、线性可分SVM

​	线性可分指的是凸包相离，查看P201

​	当训练集线性可分，则存在规范超平面$(\omega x)+b=0$，使得：

- $(\omega a_i)+b\ge1,y_i=1$
- $(\omega a_i) +b\le -1,y_i=-1$

### 2.1、普通支持向量

​	上面式中满足$(\omega a_i)+b=\pm1$的称为普通支持向量；线性可分时只有支持向量起到作用，只占到很少一部分。

​	普通支持向量之间的间隔为$\frac{2}{||\omega||}$，最优化超平面就是最大化间隔，所以转化为二次规划问题：

​	$$min\frac{1}{2} ||\omega||^2$$

​	$$s.t.y_i[(\omega a_i+b)]\ge1$$

​	之后引入拉格朗日函数，对偶，KKT条件



## 3、线性SVM

​	上面要求严格是硬间隔，这个是软间隔；在线性不可分的时候放松要求；引入松弛变量得到软化约束条件

​	$$min\frac{1}{2} ||\omega||^2+C\sum_{i=1}^{l}\xi_{i}$$

​	$$s.t.y_i[(\omega a_i+b)]\ge1-\xi_i$$

​	$s.t.\xi_i\ge 0,i=0,1,2,...$

​	其中$\xi_i$越大，要求越松。



## 4、可分SVM

​	两类重合区域过大的时候，线性的SVM就不可用了，需要从输入空间变换到高位的hilbert空间$x\rightarrow \phi(x)$把训练集转化为hilbert空间中的训练集合。

​	$T'=\{[\phi(a_i),y_i]|i=0,1,2,...\}$

​	在H空间中硬性划分训练集合T'，使用下面的二次规划问题：

​	$$min\frac{1}{2} ||\omega||^2$$

​	$$s.t.y_i[(\omega \phi(a_i)+b)]\ge1,i=0,1,2,...$$

​	核函数满足$K(a_i,a_j)=\phi(a_i)\phi(a_j)$，使用核函数可以避免在高位空间进行复杂的运算。

​	之后仍然是利用对偶问题来求解。



## 5、C-SVM

​	核函数也不能保证线性可分，还是需要软化，



## 6、调库实战

